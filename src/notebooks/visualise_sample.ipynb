{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from petastorm import make_reader\n",
    "from petastorm.predicates import in_lambda\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "SMALL_SIZE = 18\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = 'Times New Roman'\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///c:\\Users\\dafaj\\Documents\\GitHub\\InterpretTime\\data\\datasets\\synthetic\\dataParquet\\part-00003-657e95fb-a4f6-46f4-aabd-5d608270bbcb-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.abspath(\"../../data/datasets/synthetic/dataParquet/part-00003-657e95fb-a4f6-46f4-aabd-5d608270bbcb-c000.parquet\") # Set your path to the dataset in parquet format\n",
    "#\"C:\\Users\\dafaj\\Documents\\GitHub\\InterpretTime\\src\\datasets\\ecg\\dataParquet\\part-00000-72f595a7-00a8-4755-b5bf-5f4b462bb233-c000.parquet\"\n",
    "data_path = f\"file:///{data_path}\" # Add file prefix for local file system, required for petastorm\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Passed non-file path: /c:\\Users\\dafaj\\Documents\\GitHub\\InterpretTime\\data\\datasets\\synthetic\\dataParquet\\part-00003-657e95fb-a4f6-46f4-aabd-5d608270bbcb-c000.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_1012\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# Set the name(s) of the sample you want to visualise\u001b[39;00m\n\u001b[0;32m      2\u001b[0m predicate_expr \u001b[38;5;241m=\u001b[39m  in_lambda([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28;01mlambda\u001b[39;00m noun_id: noun_id\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m sample_name) \u001b[38;5;66;03m# Create a predicate to filter the dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m make_reader(data_path, predicate \u001b[38;5;241m=\u001b[39m predicate_expr) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx,sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader):\n\u001b[0;32m      5\u001b[0m         signal\u001b[38;5;241m=\u001b[39m  sample\u001b[38;5;241m.\u001b[39msignal\n",
      "File \u001b[1;32mc:\\Users\\dafaj\\anaconda3\\Lib\\site-packages\\petastorm\\reader.py:155\u001b[0m, in \u001b[0;36mmake_reader\u001b[1;34m(dataset_url, schema_fields, reader_pool_type, workers_count, pyarrow_serialize, results_queue_size, seed, shuffle_rows, shuffle_row_groups, shuffle_row_drop_partitions, predicate, rowgroup_selector, num_epochs, cur_shard, shard_count, shard_seed, cache_type, cache_location, cache_size_limit, cache_row_size_estimate, cache_extra_settings, hdfs_driver, transform_spec, filters, storage_options, zmq_copy_buffers, filesystem)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown cache_type: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cache_type))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     dataset_metadata\u001b[38;5;241m.\u001b[39mget_schema_from_dataset_url(dataset_url_or_urls, hdfs_driver\u001b[38;5;241m=\u001b[39mhdfs_driver,\n\u001b[0;32m    156\u001b[0m                                                  storage_options\u001b[38;5;241m=\u001b[39mstorage_options, filesystem\u001b[38;5;241m=\u001b[39mfilesystem)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PetastormMetadataError:\n\u001b[0;32m    158\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrently make_reader supports reading only Petastorm datasets. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo read from a non-Petastorm Parquet store use make_batch_reader\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dafaj\\anaconda3\\Lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:402\u001b[0m, in \u001b[0;36mget_schema_from_dataset_url\u001b[1;34m(dataset_url_or_urls, hdfs_driver, storage_options, filesystem)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`petastorm.unischema.Unischema` object loaded from a dataset specified by a url.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m:param dataset_url_or_urls: a url to a parquet directory or a url list (with the same scheme) to parquet files.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m:return: A :class:`petastorm.unischema.Unischema` object\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m fs, path_or_paths \u001b[38;5;241m=\u001b[39m get_filesystem_and_path_or_paths(dataset_url_or_urls, hdfs_driver,\n\u001b[0;32m    399\u001b[0m                                                      storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    400\u001b[0m                                                      filesystem\u001b[38;5;241m=\u001b[39mfilesystem)\n\u001b[1;32m--> 402\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetDataset(path_or_paths, filesystem\u001b[38;5;241m=\u001b[39mfs, validate_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, metadata_nthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# Get a unischema stored in the dataset metadata.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m stored_schema \u001b[38;5;241m=\u001b[39m get_schema(dataset)\n",
      "File \u001b[1;32mc:\\Users\\dafaj\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1858\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size, partitioning, use_legacy_dataset, pre_buffer, coerce_int96_timestamp_unit, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_metadata\u001b[38;5;241m.\u001b[39mmemory_map \u001b[38;5;241m=\u001b[39m memory_map\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_metadata\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m=\u001b[39m buffer_size\n\u001b[0;32m   1855\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pieces,\n\u001b[0;32m   1856\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partitions,\n\u001b[0;32m   1857\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_metadata_path,\n\u001b[1;32m-> 1858\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_path) \u001b[38;5;241m=\u001b[39m _make_manifest(\n\u001b[0;32m   1859\u001b[0m      path_or_paths, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs, metadata_nthreads\u001b[38;5;241m=\u001b[39mmetadata_nthreads,\n\u001b[0;32m   1860\u001b[0m      open_file_func\u001b[38;5;241m=\u001b[39mpartial(_open_dataset_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_metadata)\n\u001b[0;32m   1861\u001b[0m )\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_metadata_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_metadata_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\dafaj\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2351\u001b[0m, in \u001b[0;36m_make_manifest\u001b[1;34m(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)\u001b[0m\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m path_or_paths:\n\u001b[0;32m   2350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[1;32m-> 2351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassed non-file path: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2352\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m   2353\u001b[0m     piece \u001b[38;5;241m=\u001b[39m ParquetDatasetPiece\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m   2354\u001b[0m         path, open_file_func\u001b[38;5;241m=\u001b[39mopen_file_func)\n\u001b[0;32m   2355\u001b[0m     pieces\u001b[38;5;241m.\u001b[39mappend(piece)\n",
      "\u001b[1;31mOSError\u001b[0m: Passed non-file path: /c:\\Users\\dafaj\\Documents\\GitHub\\InterpretTime\\data\\datasets\\synthetic\\dataParquet\\part-00003-657e95fb-a4f6-46f4-aabd-5d608270bbcb-c000.parquet"
     ]
    }
   ],
   "source": [
    "sample_name = [\"sample_1012\"] # Set the name(s) of the sample you want to visualise\n",
    "predicate_expr =  in_lambda([\"noun_id\"], lambda noun_id: noun_id.astype(\"str\") in sample_name) # Create a predicate to filter the dataset\n",
    "with make_reader(data_path, predicate = predicate_expr) as reader:\n",
    "    for idx,sample in enumerate(reader):\n",
    "        signal=  sample.signal\n",
    "        name = sample.noun_id.decode(\"utf-8\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m15\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(signal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     ax[idx]\u001b[38;5;241m.\u001b[39mplot(signal[:,idx])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signal' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows= signal.shape[1],  figsize=(12,15))\n",
    "for idx in range(signal.shape[1]):\n",
    "    ax[idx].plot(signal[:,idx])\n",
    "    ax[idx].set_title(\"Feature {}\".format(idx+1), fontsize = 20)\n",
    "    ax[idx].set_xlabel(\"$n$\", fontsize = 20)\n",
    "    ax[idx].set_ylabel(\"$A$\", fontsize = 20)\n",
    "    ax[idx].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[idx].yaxis.set_tick_params(labelsize=20)\n",
    "    ax[idx].set_ylim(-1.6, 1.6)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d59d8ec59ac0ed6a91aadbfdb4fb3fa55c9157ae2de9e66f28de6726f22506ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
